# Web Data Scraper Framework

A flexible and configurable framework designed for efficient and automated web data extraction. Built with Python, it supports scraping static HTML, dynamic JavaScript-rendered websites, and web APIs, making it suitable for various data gathering tasks, particularly for roles like Virtual Assistants.

## Features

* **Multiple Scraping Strategies:**
    * **HTML Scraper:** Uses `requests` and `BeautifulSoup4` for fast scraping of static content.
    * **Dynamic Scraper:** Uses `selenium` to control a headless browser (Chrome by default) for scraping sites that rely on JavaScript.
    * **API Scraper:** Module for interacting with JSON-based web APIs (integration into UI/CLI pending).
* **Configuration-Driven:** Define scraping jobs using simple YAML files (no coding required for basic use).
* **Data Processing:** Clean, transform, validate, and restructure extracted data using configurable rules defined in the YAML config.
* **Pagination:** Automatically navigate through multiple pages of results (currently implemented for HTML Scraper).
* **Multiple Output Formats:** Save scraped data as CSV, JSON, or into an SQLite database.
* **User Interfaces:**
    * **Web UI:** A user-friendly Flask web application to create, manage (List/View/Edit/Delete), and run scraping jobs through your browser.
    * **Command-Line Interface (CLI):** Run jobs and generate sample configurations directly from your terminal using `typer`.
* **Configurable Behavior:** Control request delays, retries, user agents, and respect for `robots.txt`.
* **Logging:** Detailed logging for monitoring and debugging, stored in the `logs/` directory.

## Project Structure

```
web-data-scraper/
├── configs/               # Default location for YAML configuration files
│   └── scraping_jobs/     # Subdir for web UI generated configs
├── interfaces/            # User interfaces
│   ├── cli.py             # Command Line Interface (Typer)
│   └── web_app/           # Web Application Interface (Flask)
│       ├── app.py
│       ├── templates/
│       └── web_app.log    # Log file specific to web app
├── logs/                  # Log files generated by the scrapers/CLI
├── outputs/               # Default location for saved output data (CSV, JSON, SQLite)
├── scraper/               # Core scraping logic
│   ├── storage/           # Data storage handlers (CSV, JSON, SQLite)
│   ├── utils/             # Utility modules (config, logger, etc.)
│   ├── base_scraper.py    # Base class for scrapers
│   ├── html_scraper.py
│   ├── dynamic_scraper.py
│   ├── api_scraper.py     # (Not fully integrated yet)
│   └── data_processor.py
├── .gitignore             # Git ignore file
├── README.md              # This file
└── requirements.txt       # Python dependencies
```

## Setup

**Prerequisites:**

* Python 3.9+
* pip (Python package installer)
* Git (for version control)
* **For Dynamic Scraping:** A WebDriver compatible with your browser (e.g., `chromedriver` for Google Chrome). Ensure it's installed and accessible in your system's PATH.

**Installation:**

1.  **Clone the repository (if applicable):**
    ```bash
    git clone https://github.com/Phoenix1025/web-data-scraper.git
    cd web-data-scraper
    ```
2.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows use `.venv\Scripts\activate`
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Configuration (`config.yaml`)

Scraping jobs are defined in YAML files (e.g., placed in `configs/`). Key sections include:

* `name`: Descriptive name for the job.
* `description`: Optional description.
* `urls`: List of starting URLs.
* `dynamic`: `true` if the site requires JavaScript/Selenium, `false` otherwise.
* `selectors`: Defines how to find data:
    * `type`: `css` (default) or `xpath` (XPath currently limited).
    * `container` (Optional): Selector for the element holding all items.
    * `item`: Selector for each individual item element.
    * `fields`: Dictionary defining data points to extract:
        * `field_name: "css_selector"` (extracts text)
        * `field_name: {selector: "css_selector", attr: "attribute_name"}` (extracts attribute like `href`, `src`)
* `pagination` (Optional):
    * `next_page_selector`: CSS selector for the "Next" page link.
    * `max_pages`: Maximum number of pages to scrape.
* `processing_rules` (Optional): Define rules for cleaning/transforming data (see `DataProcessor` and sample configs).
* `output_dir`: Base directory for saving results.
* `request_delay`, `max_retries`, `user_agent`, `respect_robots`: Control scraping behavior.
* Dynamic options (`wait_for_selector`, `wait_time`, `headless`, etc.) if `dynamic: true`.

*Refer to `configs/example_config.yaml` and `configs/quotes_paged_config.yaml` for examples.*

## Usage

**1. Web Interface:**

* Start the Flask app:
    ```bash
    python -m interfaces.web_app.app
    ```
* Open your web browser to `http://127.0.0.1:5001` (or your machine's IP address on port 5001).
* **Create:** Click "Create New Job", fill out the form (URLs, selectors, etc.), and save.
* **Manage:** View, Edit, Delete, or Run existing jobs listed on the home page.
* **Run:** Click the "Run" button next to a job. Results are saved in the `outputs/` directory (within a subfolder named after the config file).

**2. Command Line Interface (CLI):**

* Navigate to the project root directory in your terminal (ensure virtual environment is active).
* Use `python -m interfaces.cli --help` to see available commands.

* **Run a Scraper Job:**
    ```bash
    python -m interfaces.cli run path/to/your_config.yaml --format [csv|json|sqlite]
    ```
    *Example:*
    ```bash
    python -m interfaces.cli run configs/quotes_paged_config.yaml --format csv
    ```
    *Specify `--no-headless` for dynamic scraping if you want to see the browser.*

* **Generate a Sample Config:**
    ```bash
    python -m interfaces.cli generate-config my_new_config.yaml
    ```

## Output

Scraped data is saved in the directory specified by `output_dir` in the configuration (defaults to `outputs/`).
* The CLI and Web App create subdirectories based on the configuration filename (without the timestamp).
* Filenames include the job name and a timestamp.
* Supported formats: CSV, JSON, SQLite.

## Contributing & Future Enhancements

(Optional: Add contribution guidelines or list planned features like:)
* Pagination for Dynamic Scraper
* API Scraper integration
* Advanced Data Processing UI
* Login/Authentication Support
* Proxy Rotation Integration
* Job Scheduling
* More Output Options (e.g., Google Sheets)
